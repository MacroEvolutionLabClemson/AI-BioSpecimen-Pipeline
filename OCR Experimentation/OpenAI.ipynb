{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.68.2-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\syed shahzad\\.conda\\envs\\labenv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: pillow in c:\\users\\syed shahzad\\.conda\\envs\\labenv\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\syed shahzad\\.conda\\envs\\labenv\\lib\\site-packages (from openai) (3.7.1)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp38-cp38-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\syed shahzad\\.conda\\envs\\labenv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\syed shahzad\\.conda\\envs\\labenv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\syed shahzad\\.conda\\envs\\labenv\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\syed shahzad\\.conda\\envs\\labenv\\lib\\site-packages (from opencv-python) (1.24.4)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\syed shahzad\\.conda\\envs\\labenv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\syed shahzad\\.conda\\envs\\labenv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\syed shahzad\\.conda\\envs\\labenv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.27.2-cp38-cp38-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\syed shahzad\\.conda\\envs\\labenv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.68.2-py3-none-any.whl (606 kB)\n",
      "   ---------------------------------------- 0.0/606.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 606.1/606.1 kB 11.1 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.9.0-cp38-cp38-win_amd64.whl (198 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 12.2 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: pydantic-core, jiter, h11, distro, annotated-types, pydantic, httpcore, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.9.0 openai-1.68.2 pydantic-2.10.6 pydantic-core-2.27.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openai opencv-python pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\syed shahzad\\.conda\\envs\\labenv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\syed shahzad\\.conda\\envs\\labenv\\lib\\site-packages (from opencv-python) (1.24.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image_path = \"sample labels/label1.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Convert to grayscale (optional, but may improve OCR results)\n",
    "#gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Save the processed image (if needed)\n",
    "#preprocessed_image_path = \"processed_handwritten.jpg\"\n",
    "#cv2.imwrite(preprocessed_image_path, gray)\n",
    "\n",
    "# Display the image\n",
    "#Image.open(preprocessed_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-audio-preview-2024-12-17\n",
      "dall-e-3\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "dall-e-2\n",
      "gpt-4o-audio-preview-2024-10-01\n",
      "gpt-4o-transcribe\n",
      "gpt-4o-realtime-preview-2024-10-01\n",
      "gpt-4o-mini-transcribe\n",
      "babbage-002\n",
      "gpt-4o-mini-tts\n",
      "tts-1-hd-1106\n",
      "text-embedding-3-large\n",
      "gpt-4\n",
      "tts-1-hd\n",
      "gpt-4o-mini-audio-preview\n",
      "o3-mini\n",
      "o3-mini-2025-01-31\n",
      "gpt-4o-audio-preview\n",
      "o1-preview-2024-09-12\n",
      "gpt-4o-realtime-preview\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-4o-mini-search-preview\n",
      "tts-1-1106\n",
      "davinci-002\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-4o-search-preview\n",
      "gpt-4-turbo\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo\n",
      "gpt-4o-mini-search-preview-2025-03-11\n",
      "gpt-4o-mini-realtime-preview\n",
      "chatgpt-4o-latest\n",
      "whisper-1\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "gpt-4-turbo-preview\n",
      "gpt-4-1106-preview\n",
      "text-embedding-ada-002\n",
      "o1-preview\n",
      "gpt-4-0613\n",
      "gpt-4-0125-preview\n",
      "gpt-4.5-preview\n",
      "o1\n",
      "gpt-4.5-preview-2025-02-27\n",
      "o1-pro\n",
      "o1-pro-2025-03-19\n",
      "gpt-4o-search-preview-2025-03-11\n",
      "o1-2024-12-17\n",
      "omni-moderation-latest\n",
      "gpt-4o-2024-11-20\n",
      "tts-1\n",
      "omni-moderation-2024-09-26\n",
      "text-embedding-3-small\n",
      "gpt-4o\n",
      "gpt-4o-mini\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-2024-08-06\n",
      "gpt-4o-mini-2024-07-18\n",
      "o1-mini\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "o1-mini-2024-09-12\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(api_key=\"secret api key\")\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "for model in models.data:\n",
    "    print(model.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text:  I20474\n",
      "331.16 SL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I20474\\n331.16 SL'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import base64\n",
    "import mimetypes\n",
    "\n",
    "client = OpenAI(api_key='secret api key')\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    # Guess the MIME type of the image\n",
    "    mime_type, _ = mimetypes.guess_type(image_path)\n",
    "    \n",
    "    if not mime_type or not mime_type.startswith('image'):\n",
    "        raise ValueError(\"The file type is not recognized as an image\")\n",
    "    \n",
    "    # Read the image binary data\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    # Format the result with the appropriate prefix\n",
    "    image_base64 = f\"data:{mime_type};base64,{encoded_string}\"\n",
    "    \n",
    "    return image_base64\n",
    "\n",
    "\n",
    "def transcribe_image(image_path):\n",
    "\n",
    "    base64_string = image_to_base64(image_path)\n",
    "    # Make an API call to submit the image for transcription\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Manually transcribe this handwriting. Just tell me your answer, do not write anything else in your response, other than what's in the text\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": base64_string,\n",
    "                        \"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "    extracted_text = response.choices[0].message.content\n",
    "    print(\"Extracted Text: \", extracted_text)\n",
    "    return extracted_text\n",
    "\n",
    "# Example usage\n",
    "image_path = 'label16.jpg'\n",
    "transcribe_image(image_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
